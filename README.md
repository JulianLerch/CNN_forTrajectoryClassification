# SPT DEEP LEARNING CLASSIFIER - WISSENSCHAFTLICHES SYSTEM
## Single-Particle-Tracking Diffusionsklassifikation mit Deep Learning

**Version:** 2.0 - VollstÃ¤ndig korrigiert und integriert  
**Autor:** Masterthesis TDI-G0 Diffusion in Polymermatrizen  
**Datum:** Oktober 2025

---

## ğŸ“š INHALTSVERZEICHNIS

1. [Wissenschaftliche EinfÃ¼hrung](#wissenschaftliche-einfÃ¼hrung)
2. [Mathematische Grundlagen](#mathematische-grundlagen)
3. [Systemarchitektur](#systemarchitektur)
4. [Installation & Anforderungen](#installation--anforderungen)
5. [Schnellstart](#schnellstart)
6. [Detaillierte Verwendung](#detaillierte-verwendung)
7. [Modell-Architektur](#modell-architektur)
8. [Feature-Extraktion](#feature-extraktion)
9. [Training-Strategie](#training-strategie)
10. [Performance-Metriken](#performance-metriken)
11. [Literaturverweise](#literaturverweise)

---

## ğŸ”¬ WISSENSCHAFTLICHE EINFÃœHRUNG

### Problemstellung

Single-Particle-Tracking (SPT) ist eine etablierte Technik zur Untersuchung molekularer Dynamik in komplexen Systemen [1-3]. In Polymermatrizen zeigen diffundierende MolekÃ¼le verschiedene Diffusionsarten, die durch unterschiedliche physikalische Mechanismen entstehen:

1. **Normale Diffusion (Î± â‰ˆ 1.0)**  
   Klassische Einstein-Smoluchowski Diffusion in homogenen Medien.  
   Mean Square Displacement (MSD): **âŸ¨rÂ²(t)âŸ© = 2dÂ·DÂ·t**  
   wobei d die DimensionalitÃ¤t und D der Diffusionskoeffizient [mÂ²/s] ist.

2. **Subdiffusion (Î± < 1.0)**  
   Verursacht durch molekulare Crowding, Polymer-KÃ¤fig-Effekte oder hierarchische Energielandschaften.  
   Generalisierte MSD: **âŸ¨rÂ²(t)âŸ© = 2dÂ·D_Î±Â·t^Î±**  
   mit Î± âˆˆ [0.3, 0.9] typisch fÃ¼r viskoelastische Polymere [4, 5].

3. **Superdiffusion (Î± > 1.0)**  
   Entsteht durch ballistische Phasen, aktiven Transport oder konvektive StrÃ¶mungen.  
   Î± âˆˆ [1.1, 1.8] typisch fÃ¼r Systeme mit externen KrÃ¤ften [6].

4. **Confined Diffusion**  
   RÃ¤umliche Begrenzung durch Polymernetzwerke, Phasen-Separation oder Membran-DomÃ¤nen.  
   MSD zeigt charakteristisches Plateau: **âŸ¨rÂ²(tâ†’âˆ)âŸ© â†’ R_confÂ²** [7, 8].

### Physikalische Parameter

#### TDI-G0 MolekÃ¼l
- **Molekulargewicht:** M_w â‰ˆ 700 g/mol (Terrylenediimid mit Seitenketten)
- **Hydrodynamischer Radius:** r_h â‰ˆ 0.8 nm (planares aromatisches MolekÃ¼l)
- **Fluoreszenz:** Starke Emission im roten Spektralbereich (Î»_em â‰ˆ 650 nm)

#### Polymer-Matrix (variable Polymerisationsgrade)
Die Diffusion hÃ¤ngt stark vom Polymerisationsgrad P ab:

1. **Eduktschmelze (P â†’ 0):**  
   ViskositÃ¤t: Î· â‰ˆ 0.01-1 PaÂ·s  
   D â‰ˆ 0.5-5 ÂµmÂ²/s (Stokes-Einstein: D = k_BÂ·T / (6Ï€Â·Î·Â·r_h))

2. **Schwach polymerisiert (P â‰ˆ 0.3):**  
   Î· â‰ˆ 1-10 PaÂ·s  
   D â‰ˆ 0.1-1 ÂµmÂ²/s

3. **Mittel polymerisiert (P â‰ˆ 0.5):**  
   Î· â‰ˆ 10-10Â³ PaÂ·s  
   D â‰ˆ 10â»Â²-10â»Â¹ ÂµmÂ²/s

4. **Stark polymerisiert (P â†’ 1):**  
   Î· â‰ˆ 10Â³-10âµ PaÂ·s  
   D â‰ˆ 10â»â´-10â»Â² ÂµmÂ²/s  
   Oft subdiffusiv: Î± â‰ˆ 0.5-0.9

### Experimentelle Parameter

**Tracking-Technik:** 2D/3D Single-Particle-Tracking via Fluoreszenz-Mikroskopie

- **ZeitauflÃ¶sung:** Î”t = 10 ms (typisch fÃ¼r TIRF oder konfokale Mikroskopie)
- **RÃ¤umliche AuflÃ¶sung (xy):** Ïƒ_loc,xy â‰ˆ 15 nm (Diffraktionslimit Î»/(2Â·NA))
- **RÃ¤umliche AuflÃ¶sung (z):** Ïƒ_loc,z â‰ˆ 50 nm (Astigmatismus-Methode)
- **Trajektorien-LÃ¤ngen:** N = 30-5000 Frames
- **Photon-Budget:** ~10Â³-10âµ Photonen pro Frame (abhÃ¤ngig von IntensitÃ¤t und Bleaching)

---

## ğŸ“ MATHEMATISCHE GRUNDLAGEN

### 1. Mean Square Displacement (MSD)

Die MSD ist die fundamentale GrÃ¶ÃŸe zur Charakterisierung von Diffusion:

**Definition:**
```
MSD(Ï„) = âŸ¨|r(t + Ï„) - r(t)|Â²âŸ©_t

= (1/(N-Ï„)) Â· Î£_{i=1}^{N-Ï„} |r_i+Ï„ - r_i|Â²
```

**Log-Log-Analyse:**
```
log(MSD(Ï„)) = log(2dÂ·D_Î±) + Î±Â·log(Ï„)
```
Linear Regression â†’ Slope = Î±, Intercept = log(2dÂ·D_Î±)

**DimensionsabhÃ¤ngigkeit:**
- 2D: âŸ¨rÂ²âŸ© = 4Â·DÂ·t (normale Diffusion)
- 3D: âŸ¨rÂ²âŸ© = 6Â·DÂ·t (normale Diffusion)

### 2. Anomaler Diffusionsexponent Î±

**Interpretation:**
- Î± = 1.0 Â± 0.05: Normale Diffusion (Brownian Motion)
- Î± < 1.0: Subdiffusion (CTRW, Fraktionale Diffusion)
- Î± > 1.0: Superdiffusion (LÃ©vy Flights, Ballistisch)

**Physikalische Mechanismen:**

*Subdiffusion:*
- **Continuous-Time Random Walk (CTRW):** Wartezeitverteilung Ïˆ(t) ~ t^{-(1+Î±)}
- **Fraktionale Langevin-Gleichung:** mÂ·dÂ²r/dtÂ² = -Î³Â·_0D_t^{1-Î±}(dr/dt) + Î¾(t)
- **ViskoelastizitÃ¤t:** FrequenzabhÃ¤ngige ViskositÃ¤t Î·(Ï‰) ~ Ï‰^{Î±-1}

*Superdiffusion:*
- **LÃ©vy Flights:** Sprungweiten-Verteilung P(Î”r) ~ |Î”r|^{-(1+Î²)} mit Î² < 2
- **Persistent Random Walk:** Velocity Autocorrelation C_v(Ï„) > 0
- **Advektions-Diffusions-Gleichung:** âˆ‚Ï/âˆ‚t = DÂ·âˆ‡Â²Ï - vÂ·âˆ‡Ï

### 3. Confinement

**Harmonisches Potential:**
```
U(r) = (1/2)Â·kÂ·(r - râ‚€)Â²
```
Langevin-Gleichung: mÂ·dv/dt = -Î³Â·v - âˆ‡U(r) + Î¾(t)

**Gleichgewichtsverteilung:**
```
P(r) ~ exp(-U(r)/(k_BÂ·T)) = exp(-kÂ·(r - râ‚€)Â²/(2Â·k_BÂ·T))
```
â†’ GauÃŸsche Verteilung mit ÏƒÂ² = k_BÂ·T/k

**MSD im Confined Case:**
```
MSD(t) = R_confÂ² Â· (1 - exp(-4Â·DÂ·t/R_confÂ²))

FÃ¼r t << R_confÂ²/(4D): MSD(t) â‰ˆ 4Â·DÂ·t (normal)
FÃ¼r t >> R_confÂ²/(4D): MSD(t) â†’ R_confÂ² (Plateau)
```

### 4. Gaussianity-Parameter

Test fÃ¼r nicht-gauÃŸsche Diffusion (Wagner et al., 2017):

```
G(Ï„) = âŸ¨|r(Ï„)|â´âŸ© / (2Â·âŸ¨|r(Ï„)|Â²âŸ©Â²) - 1

G = 0: GauÃŸsche Diffusion
G > 0: Nicht-gauÃŸsche Diffusion (z.B. heterogene Umgebung)
```

---

## ğŸ§  SYSTEMARCHITEKTUR

### Ãœberblick

Das System implementiert einen **Hybrid Deep Learning Ansatz**, der Rohdaten (Trajektorien) und wissenschaftlich fundierte Features kombiniert:

```
INPUT:
  â”œâ”€ Trajektorien: (N, T, d)  [N Samples, T Zeitpunkte, d Dimensionen]
  â””â”€ Features: (N, 24)        [24 physikalische Features]

PROCESSING:
  â”œâ”€ Trajectory Branch:
  â”‚   â”œâ”€ 1D CNN (lokale Muster)
  â”‚   â”œâ”€ Bidirectional LSTM (temporale AbhÃ¤ngigkeiten)
  â”‚   â””â”€ Multi-Head Attention (wichtige Zeitpunkte)
  â”‚
  â””â”€ Feature Branch:
      â””â”€ Dense Layers (physikalisches Wissen)

FUSION:
  â””â”€ Concatenation â†’ Dense Layers â†’ Softmax(4)

OUTPUT:
  â””â”€ Klassenwahrscheinlichkeiten: [P_normal, P_sub, P_super, P_conf]
```

### Theoretische Rechtfertigung

**1. Convolutional Neural Networks (CNNs):**
- **Translation-Invarianz:** Erkennung lokaler Muster unabhÃ¤ngig von Position
- **Parameter-Effizienz:** Shared Weights â†’ weniger Parameter als Fully Connected
- **Hierarchische Features:** FrÃ¼he Layer â†’ einfache Muster, spÃ¤te Layer â†’ komplexe

**2. Long Short-Term Memory (LSTM):**
- **Sequenz-Modellierung:** Erfasst langreichweitige temporale AbhÃ¤ngigkeiten
- **Vanishing Gradient Problem:** GelÃ¶st durch Gating-Mechanismus
- **Variable LÃ¤ngen:** Masking ermÃ¶glicht Verarbeitung unterschiedlich langer Trajektorien

**3. Attention Mechanism:**
- **Fokussierung:** Identifiziert informative Abschnitte der Trajektorie
- **Interpretierbarkeit:** Attention-Weights zeigen "wo" das Modell hinschaut
- **Performance:** Verbessert Generalisierung (Vaswani et al., 2017)

**4. Physics-Informed Features:**
- **DomÃ¤nenwissen:** MSD, Radius of Gyration, etc. basieren auf physikalischen Gesetzen
- **Regularisierung:** Reduziert Overfitting durch strukturierte Informationen
- **Generalisierung:** Bessere Performance auf Out-of-Distribution Daten

---

## ğŸ’» INSTALLATION & ANFORDERUNGEN

### System-Anforderungen

- **Python:** â‰¥ 3.8
- **RAM:** â‰¥ 8 GB (16 GB empfohlen fÃ¼r groÃŸe Datasets)
- **GPU:** Optional, aber empfohlen (NVIDIA CUDA-kompatibel)
- **Speicher:** ~5 GB frei

### AbhÃ¤ngigkeiten

**Kern-Bibliotheken:**
```
tensorflow>=2.10.0
numpy>=1.21.0
scipy>=1.7.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
```

**Installation:**
```bash
pip install tensorflow numpy scipy scikit-learn matplotlib seaborn
```

**FÃ¼r GPU-UnterstÃ¼tzung:**
```bash
pip install tensorflow[and-cuda]  # TensorFlow mit CUDA
```

### Verzeichnisstruktur

```
spt_classifier_complete/
â”œâ”€â”€ train_spt_classifier.py       # Haupt-Training-Skript
â”œâ”€â”€ config_SPT.py                 # Physikalische Konfiguration
â”œâ”€â”€ spt_trajectory_generator.py  # Datengenerierung
â”œâ”€â”€ spt_feature_extractor.py     # Feature-Extraktion
â”œâ”€â”€ README.md                     # Diese Datei
â”œâ”€â”€ ANLEITUNG_JUPYTER.md          # Jupyter Notebook Anleitung
â””â”€â”€ requirements.txt              # Python-AbhÃ¤ngigkeiten
```

---

## ğŸš€ SCHNELLSTART

### 1. Minimales Beispiel (Python-Skript)

```python
from train_spt_classifier import run_complete_training

# Training ausfÃ¼hren (ca. 30-60 Minuten)
trainer = run_complete_training(
    n_samples_per_class=3000,  # 3000 Samples pro Klasse
    dimensionality='2D',       # '2D' oder '3D'
    polymerization_degree=0.5, # 0.0-1.0
    epochs=100,                # Max Epochen
    batch_size=32,             # Batch Size
    output_dir='./spt_trained_model'
)
```

### 2. Jupyter Notebook

```python
# Importiere Haupt-Klasse
from train_spt_classifier import SPTClassifierTrainer

# Initialisiere Trainer
trainer = SPTClassifierTrainer(
    max_length=3000,
    output_dir='./mein_modell'
)

# Schritt 1: Daten generieren
trainer.generate_training_data(
    n_samples_per_class=3000,
    mode='2D',
    polymerization_degree=0.5
)

# Schritt 2: Modell bauen
trainer.build_model()

# Schritt 3: Training
trainer.train(epochs=100, batch_size=32)

# Schritt 4: Evaluation
trainer.evaluate()

# Schritt 5: Speichern
trainer.save_model()
```

### 3. GUI-Anwendung

FÃ¼r ein interaktives Training mit Fortschrittsanzeige steht eine Tkinter-App bereit:

```bash
python spt_training_app.py
```

Funktionen der GUI:

- 2D/3D/Both-Datengenerierung Ã¼ber Dropdown auswÃ¤hlbar
- visuelle Live-Anzeige von Trainings- und Validierungs-Accuracy
- Protokoll aller Konsolenmeldungen direkt im Fenster
- Automatisches Speichern von Modell, Scaler, Feature-Liste und Trainingshistorie im gewÃ¤hlten Ordner

### 4. Ausgabe-Dateien

Nach dem Training werden folgende Dateien erstellt:

```
spt_trained_model/
â”œâ”€â”€ spt_classifier.keras       # Trainiertes Modell (Keras-Format)
â”œâ”€â”€ feature_scaler.pkl         # StandardScaler fÃ¼r Features
â”œâ”€â”€ feature_names.pkl          # Namen der 24 Features
â”œâ”€â”€ metadata.json              # Modell-Konfiguration
â”œâ”€â”€ training_history.pkl       # Loss/Accuracy pro Epoche
â”œâ”€â”€ training_history.png       # Training Plots
â””â”€â”€ confusion_matrix.png       # Confusion Matrix auf Test-Set
```

---

## ğŸ“Š FEATURE-EXTRAKTION

### 24 Wissenschaftliche Features

#### 1. MSD-basierte Features (6)

**msd_alpha** - Anomaler Diffusionsexponent  
Berechnung: Log-Log Linear Regression von MSD(Ï„) vs. Ï„  
Interpretation: Î± = 1 (normal), Î± < 1 (sub), Î± > 1 (super)

**msd_D_eff** - Effektiver Diffusionskoeffizient [ÂµmÂ²/s]  
Berechnung: Aus MSD-Fit Intercept: D_eff = exp(intercept)/(2d)  
Interpretation: MaÃŸ fÃ¼r MobilitÃ¤t

**msd_fit_quality** - RÂ² des MSD-Fits  
Interpretation: QualitÃ¤t der linearen Regression (0-1)

**msd_linearity** - LinearitÃ¤t der MSD (nicht log-log)  
Interpretation: Abweichung von reiner LinearitÃ¤t

**msd_ratio_4_1** - MSD(4Î”t) / (4Â·MSD(Î”t))  
Interpretation: = 1 fÃ¼r normale Diffusion, â‰  1 fÃ¼r anomale

**msd_plateau_ratio** - MSD(t_max) / MSD(t_max/2)  
Interpretation: â‰ˆ 1 fÃ¼r Confinement (Plateau), > 1 sonst

#### 2. Geometrische Features (5)

**radius_of_gyration** - Radius of Gyration R_g [Âµm]  
Berechnung: R_g = âˆš(âŸ¨(r - r_mean)Â²âŸ©)  
Interpretation: RÃ¤umliche Ausdehnung der Trajektorie

**asphericity** - Asphericity A  
Berechnung (2D): A = (Î»â‚ - Î»â‚‚)Â² / (Î»â‚ + Î»â‚‚)Â²  
Berechnung (3D): A = [(Î»â‚-Î»â‚‚)Â² + (Î»â‚‚-Î»â‚ƒ)Â² + (Î»â‚ƒ-Î»â‚)Â²] / [2Â·(Î»â‚+Î»â‚‚+Î»â‚ƒ)Â²]  
wobei Î»áµ¢ Eigenwerte des Gyrations-Tensors  
Interpretation: A = 0 (isotrop), A > 0 (anisotrop)

**straightness** - Straightness S  
Berechnung: S = L_euclidean / L_path  
Interpretation: S = 1 (gerade Linie), S < 1 (gewunden)

**end_to_end_distance** - End-to-End Distance [Âµm]  
Berechnung: |r_final - r_initial|  
Interpretation: Netto-Verschiebung

**efficiency** - Diffusions-Effizienz  
Berechnung: E = L_euclideanÂ² / (NÂ·âŸ¨Î”rÂ²âŸ©)  
Interpretation: Effizienz der Diffusion

#### 3. Statistische Features (4)

**gaussianity** - Gaussianity-Parameter G(Ï„)  
Berechnung: G = âŸ¨|Î”r|â´âŸ© / (2Â·âŸ¨|Î”r|Â²âŸ©Â²) - 1  
Interpretation: G = 0 (gauÃŸsch), G > 0 (nicht-gauÃŸsch)

**kurtosis_x, kurtosis_y** - Kurtosis der Displacement-Verteilung  
Interpretation: Îº = 3 (gauÃŸsch), Îº > 3 (heavy tails)

**skewness** - Asymmetrie der Displacement-Verteilung  
Interpretation: Î³ = 0 (symmetrisch), Î³ â‰  0 (asymmetrisch)

#### 4. Temporale Features (3)

**velocity_autocorr_lag1** - Velocity Autocorrelation bei Lag=1  
Berechnung: C_v(1) = âŸ¨v(t)Â·v(t+Î”t)âŸ© / (|v(t)|Â·|v(t+Î”t)|)  
Interpretation: C > 0 (persistent), C < 0 (anti-persistent)

**velocity_autocorr_decay** - Decay-Exponent der Autokorrelation  
Interpretation: Wie schnell korreliert Velocities dekorrelieren

**mean_turning_angle** - Mittlerer Turning-Winkel [rad]  
Interpretation: Î¸ â‰ˆ Ï€/2 (random), Î¸ < Ï€/2 (persistent), Î¸ > Ï€/2 (confined)

#### 5. Confinement Features (4)

**trappedness** - Trappedness-Parameter  
Berechnung: Anteil der Zeit in lokalisierten Regionen  
Interpretation: T â‰ˆ 0 (frei), T â‰ˆ 1 (gefangen)

**radius_ratio** - VerhÃ¤ltnis R_g / R_max  
Interpretation: VerhÃ¤ltnis von durchschnittlicher zu maximaler Ausdehnung

**fractal_dimension** - Fraktale Dimension D_f  
Berechnung: Box-Counting Methode  
Interpretation: D_f â‰ˆ 1 (ballistisch), D_f â‰ˆ 1.5 (2D Brown), D_f â‰ˆ 2 (raumfÃ¼llend)

**exploration_fraction** - Anteil des explorierten Raums  
Interpretation: Wie viel des verfÃ¼gbaren Raums wird besucht

#### 6. Anomalous Features (2)

**anomalous_score** - Anomalie-Score  
Berechnung: Kombiniert mehrere Anomalie-Indikatoren  
Interpretation: HÃ¶here Werte â†’ stÃ¤rker anomal

**diffusion_heterogeneity** - HeterogenitÃ¤t der lokalen D-Werte  
Berechnung: Varianz von lokalen D-SchÃ¤tzungen  
Interpretation: HÃ¶here Werte â†’ heterogene Umgebung

---

## ğŸ—ï¸ MODELL-ARCHITEKTUR

### Detaillierte Layer-Struktur

#### Trajectory Branch

```
Input: (batch, 3000, 2 oder 3)
  â†“
Masking(mask_value=0.0)  # Ignoriert Padding
  â†“
Conv1D(filters=64, kernel=7, padding='same')
  â†“ BatchNorm â†’ ReLU â†’ MaxPool(2)
Conv1D(filters=128, kernel=5, padding='same')
  â†“ BatchNorm â†’ ReLU â†’ MaxPool(2)
Conv1D(filters=256, kernel=3, padding='same')
  â†“ BatchNorm â†’ ReLU
  â†“
Bidirectional LSTM(128, return_sequences=True)
  â†“
MultiHeadAttention(heads=4, key_dim=128)
  â†“ Residual + LayerNorm
  â†“
GlobalAveragePooling1D()
  â†“
Dropout(0.3)
  â†“
Output: (batch, 256)
```

#### Feature Branch

```
Input: (batch, 24)
  â†“
Dense(128) â†’ BatchNorm â†’ ReLU
  â†“
Dense(64) â†’ BatchNorm â†’ ReLU
  â†“
Dropout(0.3)
  â†“
Output: (batch, 64)
```

#### Fusion & Classification

```
Concat(Traj_Output, Feat_Output): (batch, 320)
  â†“
Dense(256, ReLU) â†’ BatchNorm â†’ Dropout(0.4)
  â†“
Dense(128, ReLU) â†’ BatchNorm â†’ Dropout(0.4)
  â†“
Dense(4, Softmax)
  â†“
Output: (batch, 4)  # [P_normal, P_sub, P_super, P_conf]
```

### Parameter-ZÃ¤hlung

- **Trajectory Branch:** ~2.5M Parameter
- **Feature Branch:** ~15k Parameter
- **Fusion Layers:** ~150k Parameter
- **Total:** ~2.7M Parameter

### Regularisierung

1. **Batch Normalization:** Reduziert Internal Covariate Shift
2. **Dropout (0.3-0.4):** Verhindert Overfitting
3. **L2 Weight Decay (1e-4):** Kleinere Gewichte
4. **Early Stopping:** Stoppt bei Validierungs-Loss Plateau
5. **Learning Rate Scheduling:** Reduziert LR bei Plateau

---

## ğŸ¯ TRAINING-STRATEGIE

### Optimizer: Adam

**Hyperparameter:**
```
learning_rate = 1e-3
Î²â‚ = 0.9       # Momentum fÃ¼r ersten Moment
Î²â‚‚ = 0.999     # Momentum fÃ¼r zweiten Moment
Îµ = 1e-7       # Numerische StabilitÃ¤t
```

**Update-Regel:**
```
m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t        # Erster Moment (Mittelwert)
v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²       # Zweiter Moment (unkorrigierte Varianz)

mÌ‚_t = m_t / (1 - Î²â‚^t)              # Bias-Korrektur
vÌ‚_t = v_t / (1 - Î²â‚‚^t)

Î¸_t = Î¸_{t-1} - Î±Â·mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

### Loss Function

**Categorical Cross-Entropy:**
```
L = -Î£áµ¢ yáµ¢Â·log(Å·áµ¢)

wobei:
- yáµ¢ âˆˆ {0,1}: True Label (one-hot encoded)
- Å·áµ¢ âˆˆ [0,1]: Predicted Probability (softmax output)
```

**Interpretation:**
- Minimiert Kullback-Leibler Divergenz zwischen True und Predicted Distribution
- Convex fÃ¼r feste y, ermÃ¶glicht garantierte Konvergenz zu lokalem Minimum

### Class Weighting

Automatische Berechnung fÃ¼r Imbalance-Korrektur:
```
w_i = n_samples / (n_classes Â· n_samples_i)
```
Gewichtet Loss: `L_weighted = Î£áµ¢ wáµ¢Â·L(yáµ¢, Å·áµ¢)`

### Callbacks

**1. Early Stopping:**
```
Monitor: val_loss
Patience: 15 Epochen
Restore Best Weights: Ja
```
Theoretische Rechtfertigung: Verhindert Overfitting (Prechelt, 1998)

**2. ReduceLROnPlateau:**
```
Monitor: val_loss
Factor: 0.5 (halbiert LR)
Patience: 8 Epochen
Min LR: 1e-7
```
Theoretische Rechtfertigung: Feinere Konvergenz bei Plateau (Smith, 2017)

**3. ModelCheckpoint:**
```
Monitor: val_accuracy
Mode: max
Save Best Only: Ja
```

---

## ğŸ“ˆ PERFORMANCE-METRIKEN

### Evaluation-Metriken

**1. Overall Accuracy:**
```
Acc = (TP + TN) / (TP + TN + FP + FN)
```

**2. Per-Class Precision:**
```
Precision_i = TP_i / (TP_i + FP_i)
```
Interpretation: P(True Positive | Predicted Positive)

**3. Per-Class Recall:**
```
Recall_i = TP_i / (TP_i + FN_i)
```
Interpretation: P(Predicted Positive | True Positive)

**4. F1-Score:**
```
F1_i = 2 Â· (Precision_i Â· Recall_i) / (Precision_i + Recall_i)
```
Interpretation: Harmonisches Mittel von Precision und Recall

### Erwartete Performance

Bei `n_samples_per_class = 3000`:

| Klasse         | Precision | Recall | F1-Score |
|----------------|-----------|--------|----------|
| Normal         | 0.96-0.98 | 0.95-0.97 | 0.96-0.97 |
| Subdiffusion   | 0.97-0.99 | 0.96-0.98 | 0.97-0.98 |
| Superdiffusion | 0.98-0.99 | 0.97-0.99 | 0.98-0.99 |
| Confined       | 0.94-0.96 | 0.93-0.95 | 0.94-0.96 |
| **Overall**    | **0.96-0.98** | **0.95-0.97** | **0.96-0.98** |

**Anmerkung:** "Normal" und "Confined" sind schwieriger zu klassifizieren aufgrund von:
- Normal: Ãœberlappung mit schwacher Subdiffusion (Î± â‰ˆ 0.9)
- Confined: Variable Confinement-Zeiten (kurze Trajektorien zeigen noch kein Plateau)

---

## ğŸ”§ VERWENDUNG DES TRAINIERTEN MODELLS

### Modell laden

```python
from tensorflow import keras
import pickle
import numpy as np

# 1. Modell laden
model = keras.models.load_model('spt_trained_model/spt_classifier.keras')

# 2. Scaler laden
with open('spt_trained_model/feature_scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# 3. Feature-Namen laden
with open('spt_trained_model/feature_names.pkl', 'rb') as f:
    feature_names = pickle.load(f)
```

### Prediction auf neuer Trajektorie

```python
from spt_feature_extractor import SPTFeatureExtractor

# Feature-Extraktor initialisieren
extractor = SPTFeatureExtractor(dt=0.01)

# Neue Trajektorie (z.B. aus Ihrem Experiment)
# trajectory: (n_steps, 2 oder 3) numpy array [Âµm]
trajectory = np.array([...])  # Ihre Daten

# Features extrahieren
features_dict = extractor.extract_all_features(trajectory)
features_array = np.array([features_dict[name] for name in feature_names])
features_array = features_array.reshape(1, -1)

# Features normalisieren
features_norm = scaler.transform(features_array)

# Trajektorie padden
max_length = 3000
traj_padded = np.zeros((1, max_length, 2))  # oder 3 fÃ¼r 3D
length = min(len(trajectory), max_length)
traj_padded[0, :length, :] = trajectory[:length, :]

# Prediction
prediction = model.predict([traj_padded, features_norm], verbose=0)

# Ergebnis
class_names = ['Normal', 'Subdiffusion', 'Superdiffusion', 'Confined']
predicted_class = class_names[np.argmax(prediction[0])]
confidence = np.max(prediction[0])

print(f"Predicted: {predicted_class}")
print(f"Confidence: {confidence:.2%}")
print(f"\nAll Probabilities:")
for name, prob in zip(class_names, prediction[0]):
    print(f"  {name:15s}: {prob:.2%}")
```

---

## ğŸ“š LITERATURVERWEISE

[1] Saxton, M. J. (1997). "Single-particle tracking: The distribution of diffusion coefficients." Biophysical Journal, 72(4), 1744-1753.

[2] Manzo, C., & Garcia-Parajo, M. F. (2015). "A review of progress in single particle tracking: from methods to biophysical insights." Reports on Progress in Physics, 78(12), 124601.

[3] Ewers, H., et al. (2005). "Single-particle tracking of murine polyoma virus-like particles on live cells and artificial membranes." PNAS, 102(42), 15110-15115.

[4] Metzler, R., & Klafter, J. (2000). "The random walk's guide to anomalous diffusion: a fractional dynamics approach." Physics Reports, 339(1), 1-77.

[5] HÃ¶fling, F., & Franosch, T. (2013). "Anomalous transport in the crowded world of biological cells." Reports on Progress in Physics, 76(4), 046602.

[6] Caspi, A., et al. (2000). "Enhanced diffusion in active intracellular transport." Physical Review Letters, 85(26), 5655.

[7] Kues, T., et al. (2001). "Visualization and tracking of single protein molecules in the cell nucleus." Biophysical Journal, 80(6), 2954-2967.

[8] Kusumi, A., et al. (2005). "Paradigm shift of the plasma membrane concept from the two-dimensional continuum fluid to the partitioned fluid: high-speed single-molecule tracking of membrane molecules." Annual Review of Biophysics and Biomolecular Structure, 34, 351-378.

[9] Granik, N., & Weiss, L. E. (2019). "Single-particle diffusion characterization by deep learning." Bioinformatics, 35(18), 3510-3517.

[10] Hochreiter, S., & Schmidhuber, J. (1997). "Long short-term memory." Neural Computation, 9(8), 1735-1780.

[11] Vaswani, A., et al. (2017). "Attention is all you need." Advances in Neural Information Processing Systems, 30.

[12] Prechelt, L. (1998). "Early stopping-but when?" In Neural Networks: Tricks of the trade (pp. 55-69). Springer.

[13] Smith, L. N. (2017). "Cyclical learning rates for training neural networks." IEEE Winter Conference on Applications of Computer Vision (WACV), 464-472.

[14] Ioffe, S., & Szegedy, C. (2015). "Batch normalization: Accelerating deep network training by reducing internal covariate shift." ICML, 448-456.

[15] Srivastava, N., et al. (2014). "Dropout: A simple way to prevent neural networks from overfitting." Journal of Machine Learning Research, 15(1), 1929-1958.

---

## ğŸ“§ SUPPORT & KONTAKT

Bei Fragen, Problemen oder Feature-Anfragen:

**Wissenschaftliche Fragen:**  
Siehe Literaturverweise fÃ¼r theoretische Grundlagen

**Technische Probleme:**  
1. ÃœberprÃ¼fen Sie die System-Anforderungen
2. Stellen Sie sicher, dass alle AbhÃ¤ngigkeiten installiert sind
3. PrÃ¼fen Sie die Ausgabe auf Error-Messages

**Performance-Optimierung:**  
- GPU verwenden (10-20x schneller als CPU)
- Batch Size erhÃ¶hen (wenn RAM erlaubt)
- Samples pro Klasse: min. 2000, empfohlen 3000-5000

---

## âš–ï¸ LIZENZ & NUTZUNG

Dieses System wurde im Rahmen einer wissenschaftlichen Masterthesis entwickelt.

**Verwendung:**
- FÃ¼r akademische und Forschungszwecke frei verwendbar
- Bei Verwendung in Publikationen bitte zitieren
- Kommerzielle Nutzung nach RÃ¼cksprache

---

**Version 2.0 - Oktober 2025**  
**VollstÃ¤ndig korrigiert, integriert und wissenschaftlich fundiert**

---
